{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf338e98-78bf-4554-8d2c-b19eee83e8d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--BUILDING MODEL--\n",
      "Stopwords loaded in 0.0010018348693847656 seconds\n",
      "Data loaded in 1.0081558227539062 seconds\n",
      "Converted to dataframes in 1.328453779220581 seconds\n",
      "Stopwords removed from headlines in 17.684462785720825 seconds\n",
      "Stopwords removed from descriptions in 28.30209183692932 seconds\n",
      "Articles grouped by category in 0.023021697998046875 seconds\n",
      "Overall probability calculated in 10.916871309280396 seconds\n",
      "Unique words listed in 0.39637255668640137 seconds\n",
      "Word Chart created in 0.04804348945617676 seconds\n",
      "Filling word chart totals\n",
      "25 percent done\n",
      "50 percent done\n",
      "75 percent done\n",
      "Word Chart totals filled in 69.12696123123169 seconds\n",
      "Calculating model probabilities\n",
      "25 percent done\n",
      "50 percent done\n",
      "75 percent done\n",
      "Word probabilities calculated in 107.74623727798462 seconds\n",
      "--MODEL FINISHED IN 236.6172091960907 SECONDS--\n",
      "--SORTING TEST DATA--\n",
      "25 percent done\n",
      "50 percent done\n",
      "75 percent done\n",
      "Test data sorted in 529.8107824325562 seconds\n",
      "--CALCULATING ACCURACY--\n",
      "Correct:  15098\n",
      "Total:  42018.0\n",
      "--OVERALL--\n",
      "Accuracy was 35.93221952496549 percent\n",
      "--BY CATEGORY--\n",
      "U.S. NEWS          0.000000\n",
      "COMEDY             0.190295\n",
      "PARENTING         10.085470\n",
      "WORLD NEWS         0.000000\n",
      "CULTURE & ARTS     0.862069\n",
      "TECH               0.444444\n",
      "SPORTS             3.882576\n",
      "ENTERTAINMENT     59.147727\n",
      "POLITICS          99.409116\n",
      "WEIRD NEWS         0.000000\n",
      "ENVIRONMENT        0.000000\n",
      "EDUCATION          0.000000\n",
      "CRIME              0.269179\n",
      "SCIENCE            1.418440\n",
      "WELLNESS          84.664804\n",
      "BUSINESS           0.420168\n",
      "STYLE & BEAUTY    64.216929\n",
      "FOOD & DRINK      28.066038\n",
      "MEDIA              0.000000\n",
      "QUEER VOICES       2.408702\n",
      "HOME & LIVING      9.113300\n",
      "WOMEN              2.888583\n",
      "BLACK VOICES       0.000000\n",
      "TRAVEL            43.504231\n",
      "MONEY              0.000000\n",
      "RELIGION           0.191571\n",
      "LATINO VOICES      0.000000\n",
      "IMPACT             0.147929\n",
      "WEDDINGS           6.465517\n",
      "COLLEGE            0.000000\n",
      "PARENTS            0.000000\n",
      "ARTS & CULTURE     0.000000\n",
      "STYLE              0.000000\n",
      "GREEN              0.000000\n",
      "TASTE              0.000000\n",
      "HEALTHY LIVING     0.000000\n",
      "THE WORLDPOST      0.405405\n",
      "GOOD NEWS          0.000000\n",
      "WORLDPOST          0.220264\n",
      "FIFTY              0.000000\n",
      "ARTS               0.000000\n",
      "DIVORCE            1.041667\n",
      "Name: accuracy, dtype: float64\n",
      "Accuracy calculated in 6.272982120513916 seconds\n",
      "Total time was 772.7009737491608 seconds\n"
     ]
    }
   ],
   "source": [
    "import json  # deals with json files\n",
    "import pandas as pd  # lets me frame this data\n",
    "import time  # lets me measure how long shit takes\n",
    "import string\n",
    "import math # logs\n",
    "import random # for holdout validation\n",
    "import nltk #for lemmatizing\n",
    "\n",
    "def calcOverAll(d):\n",
    "    total = len(d.index)  # total amount of articles\n",
    "   \n",
    "    probs = pd.DataFrame(columns=[\"probability\"], index=categories, dtype=\"float64\")\n",
    "    probs.fillna(0, inplace=True)\n",
    "    for row in d.itertuples(name=None):  # basically the same as the word prob chart\n",
    "        \n",
    "        probs.loc[row[3], \"probability\"] += 1  # but with the categories instead\n",
    "    for row in probs.itertuples(name=None):\n",
    "        probs.loc[row[0], \"probability\"] = probs.loc[row[0], \"probability\"] / total\n",
    "    probs = probs[\"probability\"].tolist()\n",
    "    return probs\n",
    "\n",
    "\n",
    "def calcCate(w, p):\n",
    "    # get the words from the text\n",
    "    words = []\n",
    "    w = w.lower()  # lowercase\n",
    "    for j in w.split():\n",
    "        j = j.translate(str.maketrans(\"\", \"\", string.punctuation))  # remove punctuation\n",
    "        if ( not j.isdigit() and j not in stopwords):  # if it  isn't a number or a stopword\n",
    "            words.append(lemma.lemmatize(j))  # add it to the list after lemmatizing it\n",
    "    words = list(set(words))  # remove duplicates\n",
    "    words.insert(0, 'overAll') # add overall to the columns\n",
    "    # create df to hold category probabilities\n",
    "    i = p.columns.values.tolist() #create list of categories\n",
    "    probabilities = pd.DataFrame(index=i, columns=words) #index = categories, columns = each word\n",
    "    # fill in each probability\n",
    "    #fills in overall probability\n",
    "    \n",
    "    for row in probabilities.itertuples():\n",
    "        for word in words:\n",
    "            try:\n",
    "                p.at[word, row[0]] #check that the word is in the model\n",
    "            except:\n",
    "                probabilities.drop(word, axis=1, inplace=True)  #if the word can't be found, get rid of it \n",
    "                words.remove(word)\n",
    "            else:\n",
    "                probabilities.at[row[0], word] = math.log(p.at[word, row[0]]) #find the probability of the word appearing in that category\n",
    "\n",
    "\n",
    "    totals = [] #get the totals for each category\n",
    "    total = 0\n",
    "    for row in probabilities.itertuples(name=None):\n",
    "        total = 0\n",
    "        i = 1\n",
    "        while i < len(row):\n",
    "            total += row[i]\n",
    "            i += 1\n",
    "        totals.append(total)  \n",
    "\n",
    "    probabilities['totals'] = totals # add them to the dataframe\n",
    "    \n",
    "    max = -1000000000000 #find the category with the largest total\n",
    "    for elem in probabilities['totals']:\n",
    "        if elem > max:\n",
    "            max = elem\n",
    "            category = probabilities.loc[probabilities['totals'] == elem].index[0]\n",
    "    \n",
    "    return category # return the category with the highest likelihood\n",
    "\n",
    "#setting up lemmatizing\n",
    "#code from https://stackoverflow.com/questions/24647400/what-is-the-best-stemming-method-in-python \n",
    "lemma = nltk.wordnet.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get stopwords into a list\n",
    "initStart = time.time()\n",
    "start = time.time()\n",
    "print(\"--BUILDING MODEL--\")\n",
    "file = open(\"stopwords.txt\", \"r\")  # opens the stopwords file\n",
    "stopwords = []  # init list to hold stopwords\n",
    "for line in file:  # iterates through each line in the file\n",
    "    stopwords.append(line.replace(\"\\n\", \"\"))  # adds each line to the list\n",
    "file.close()  # closes the file\n",
    "finish = time.time()\n",
    "print(\"Stopwords loaded in %s seconds\" % (time.time() - start))\n",
    "\n",
    "# get the data out of the json file\n",
    "start = time.time()\n",
    "file = open(\"News_Category_Dataset_v3.json\", \"r\")  # opens the json file as file\n",
    "testSrc = []  # init list to hold the majority of the data\n",
    "modelSrc = []  # init list to hold data to test with\n",
    "x = 0\n",
    "for line in file:  # iterates through each line in the file\n",
    "    j = random.randint(1,5)\n",
    "    if j == 1: #roughly 20%\n",
    "        testSrc.append(json.loads(line))  # add it to the test data\n",
    "    else:\n",
    "        modelSrc.append(json.loads(line))  # add it to the model data\n",
    "file.close()  # closes the file\n",
    "print(\"Data loaded in %s seconds\" % (time.time() - start))\n",
    "\n",
    "# get the data into proper dataframes\n",
    "start = time.time()\n",
    "model = pd.DataFrame.from_dict(modelSrc)  # converts model data into dataframe\n",
    "test = pd.DataFrame.from_dict(testSrc)  # converts test data into dataframe\n",
    "data = pd.read_json('News_Category_Dataset_v3.json', lines=True) # gets the whole data set\n",
    "\n",
    "print(\"Converted to dataframes in %s seconds\" % (time.time() - start))\n",
    "\n",
    "# stopword removal code from https://stackoverflow.com/questions/29523254/python-remove-stop-words-from-pandas-dataframe\n",
    "# punctuation removal from https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "# remove the stopwords from headline\n",
    "start = time.time()\n",
    "model[\"headline\"] = model[\"headline\"].apply(lambda i: [word.lower() for word in i.split()])  # make all words lowercase\n",
    "model[\"headline\"] = model[\"headline\"].apply(lambda i: [word.translate(str.maketrans(\"\", \"\", string.punctuation)) for word in i])  # remove punctuation\n",
    "model[\"headline\"] = model[\"headline\"].apply(lambda i: [word for word in i if not word.isdigit()])  # remove numbers\n",
    "model[\"headline\"] = model[\"headline\"].apply(lambda i: [word for word in i if word not in stopwords])  # remove stopwords\n",
    "model[\"headline\"] = model[\"headline\"].apply(lambda i: [lemma.lemmatize(word) for word in i])  # lemmatize\n",
    "print(\"Stopwords removed from headlines in %s seconds\" % (time.time() - start))\n",
    "\n",
    "# remove the stopwords from description\n",
    "start = time.time()\n",
    "model[\"short_description\"] = model[\"short_description\"].apply(lambda i: [word.lower() for word in i.split()])\n",
    "model[\"short_description\"] = model[\"short_description\"].apply(lambda i: [word.translate(str.maketrans(\"\", \"\", string.punctuation)) for word in i])\n",
    "model[\"short_description\"] = model[\"short_description\"].apply(lambda i: [word for word in i if not word.isdigit()])\n",
    "model[\"short_description\"] = model[\"short_description\"].apply(lambda i: [word for word in i if word not in stopwords])\n",
    "model[\"short_description\"] = model[\"short_description\"].apply(lambda i: [lemma.lemmatize(word) for word in i])  # lemmatize\n",
    "print(\"Stopwords removed from descriptions in %s seconds\" % (time.time() - start))\n",
    "\n",
    "# list unique categories\n",
    "start = time.time()\n",
    "categories = pd.Index(data.category.unique())\n",
    "print(\"Articles grouped by category in %s seconds\" % (time.time() - start))\n",
    "# get overall probability\n",
    "start = time.time()\n",
    "overAll = calcOverAll(model)\n",
    "print(\"Overall probability calculated in %s seconds\" % (time.time() - start))\n",
    "\n",
    "# list unique words\n",
    "start = time.time()\n",
    "words = []\n",
    "for i in model[\"short_description\"]:\n",
    "    for j in i:\n",
    "        words.append(j)\n",
    "for i in model[\"headline\"]:\n",
    "    for j in i:\n",
    "        words.append(j)\n",
    "words = pd.Index(list(set(words)))\n",
    "print(\"Unique words listed in %s seconds\" % (time.time() - start))\n",
    "\n",
    "# create a dataframe of the unique words and categories\n",
    "start = time.time()\n",
    "modelP = pd.DataFrame(columns=categories, index=words, dtype=\"float64\")\n",
    "modelP.fillna(1, inplace=True)# laplace smoothing \n",
    "print(\"Word Chart created in %s seconds\" % (time.time() - start))\n",
    "# fill the probability dataframe\n",
    "start = time.time()\n",
    "print(\"Filling word chart totals\")\n",
    "i = 0\n",
    "for row in model.itertuples(name=None):  # for every row in the source data\n",
    "    for word in row[2]:  # for every word in the headline\n",
    "        modelP.at[word, row[3]] += 1  # add one to the corresponding word/category in the prob table\n",
    "    for word in row[4]:  # same for the description\n",
    "        modelP.at[word, row[3]] += 1\n",
    "    i += 1\n",
    "    if i == 41905:\n",
    "        print(\"25 percent done\")\n",
    "    if i == 83811:\n",
    "        print(\"50 percent done\")\n",
    "    if i == 125716:\n",
    "        print(\"75 percent done\")\n",
    "print(\"Word Chart totals filled in %s seconds\" % (time.time() - start))\n",
    "\n",
    "# remove weird row with blank index\n",
    "modelP.drop(index=\"\", inplace=True)  \n",
    "\n",
    "# calculate the probabilities that each word appears in each category\n",
    "start = time.time()\n",
    "print(\"Calculating model probabilities\")\n",
    "j = 0\n",
    "for row in modelP.itertuples(name=None):  # for each row\n",
    "    # get the total amount of times that word appears\n",
    "    total = 0\n",
    "    i = 1\n",
    "    while i < len(row):\n",
    "        total += row[i]\n",
    "        i += 1\n",
    "    # get how often it appears per column (probability that word appears in each category)\n",
    "    for col in modelP:\n",
    "        modelP.at[row[0], col] = modelP.at[row[0], col] / total\n",
    "    j += 1\n",
    "    if j == 26962:\n",
    "        print(\"25 percent done\")\n",
    "    if j == 53925:\n",
    "        print(\"50 percent done\")\n",
    "    if j == 80887:\n",
    "        print(\"75 percent done\")\n",
    "print(\"Word probabilities calculated in %s seconds\" % (time.time() - start))\n",
    "\n",
    "# add the overall probabilities to modelP\n",
    "overall = pd.DataFrame([overAll], columns=modelP.columns, index=['overAll']) \n",
    "modelP = pd.concat([overall, modelP])\n",
    "print(\"--MODEL FINISHED IN %s SECONDS--\" % (time.time() - initStart))\n",
    "# calculate which article belongs in which category\n",
    "start = time.time()\n",
    "print(\"--SORTING TEST DATA--\")\n",
    "modelProbs = pd.DataFrame(columns=[\"category\"])\n",
    "i = 0\n",
    "for row in test.itertuples(name=None):\n",
    "    modelProbs.loc[len(modelProbs.index)] = [calcCate((row[2] +' '+  row[4]), modelP)]\n",
    "    i += 1\n",
    "    if i == 10476:\n",
    "        print(\"25 percent done\")\n",
    "    if i == 20952:\n",
    "        print(\"50 percent done\")\n",
    "    if i == 31428:\n",
    "        print(\"75 percent done\")\n",
    "print(\"Test data sorted in %s seconds\" % (time.time() - start))\n",
    "\n",
    "start = time.time()\n",
    "print(\"--CALCULATING ACCURACY--\")\n",
    "correct = 0\n",
    "incorrect = 0\n",
    "accuracy = pd.DataFrame(columns=data.category.unique(), index = ['correct','incorrect', 'total', 'accuracy']) # create a dataframe to hold accuracy information (correct, total, and accuracy per category)\n",
    "accuracy.fillna(0.0, inplace=True) # fill each elem with 0.0\n",
    "\n",
    "# fill the chart\n",
    "for i in modelProbs.itertuples(): # for every article\n",
    "    if i[1] == test.loc[i[0], 'category']: # if the predicted category matched the actual category\n",
    "        correct += 1 # increase the total correct score\n",
    "        total += 1\n",
    "        accuracy.loc['correct', i[1]] += 1 # increase the correct score in that category\n",
    "        accuracy.loc['total', i[1]] += 1 # increase the total amount of predictions in that category\n",
    "    else:\n",
    "        total += 1 # increase the total \n",
    "        incorrect += 1 # increase the total incorrect\n",
    "        accuracy.loc['incorrect', test.loc[i[0],'category']] += 1\n",
    "        accuracy.loc['total', test.loc[i[0],'category']] += 1 # increase the total for that category\n",
    "score = (correct/total)*100 # calc the overall accuracy\n",
    "for category in accuracy: #calc the accuracy for each category\n",
    "    accuracy.loc['accuracy', category] = ((accuracy.loc['correct', category] / accuracy.loc['total', category])*100)\n",
    "    if accuracy.loc['accuracy', category] == 0.0: # if a categoryy had no predictions mark it as so\n",
    "        accuracy.loc['accuracy', category] = 0\n",
    "accuracy.fillna(0, inplace=True) # same here\n",
    "print(\"Correct: \", correct)\n",
    "print(\"Total: \", total)\n",
    "print(\"--OVERALL--\")\n",
    "print(\"Accuracy was %s percent\" % score)\n",
    "print(\"--BY CATEGORY--\")\n",
    "print(accuracy.loc['accuracy'])\n",
    "print(\"Accuracy calculated in %s seconds\" % (time.time() - start))\n",
    "print(\"Total time was %s seconds\" % (time.time() - initStart))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07fa671-3afe-485e-9036-1adb5845f59a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
